---
title: "Report Exercise: re_stepwise"
author: "Patrick Bigler"
date: "2023-04-24"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 75
---

Course: Applied Geo-data Science at the University of Bern (Institute of
Geography)

Supervisor: Prof. Dr. Benjamin Stocker

Adviser: Dr. Koen Hufkens, Pepa Aran, Pascal Schneider

Further information: <https://geco-bern.github.io/agds/>

[Do you have questions about the workflow? Contact the Author:]{.underline}

Author: Bigler Patrick (patrick.bigler1\@students.unibe.ch)

Matriculation number: 20-100-178

Reference: Report Exercise 4 (Chapter 8)

# Introduction

## Objectives

Modeling is an important aspect of data science. But there are different
types of models with different levels of complexity. With a realistic
problem, the following goals should be trained:

-   Understand the basics of regression and classification models.

-   Fit linear and logistic regression models in R.

-   Choose and calculate relevant model performance metrics.

-   Evaluate and compare regression models.

-   Detect data outliers.

-   Select best predictive variables.

## Theory

In environmental and climate science, dozens of variables have been
measured. With a model, we want to try to predict a target. For that, we
use a linear model. Unfortunately, we do not know how many variables and
which variables we should use. We only know that we try to find a formula
like this:

$$
\hat{y}=a_0 +\sum_{n=1}^{N}\hat{a}_n*x_n
$$

# Methods

To build up the best linear regression model, we could, of course, simply
try all combinations by permutation. But that would be a waste of
resources. However, we cannot avoid a try-and-error approach. But we would
like to reduce the number of combinations. We implement an algorithm that
works as follows:

1.  Let $p\in[1,N]$ and a predictor. Let $N\in[1,max(variable)]$. N could
    be all the variables we have or only a subset of them. Our algorithm
    will decide this (at the start, we did not know).

2.  The algorithm takes p and creates a linear model. The p with the
    highest RSQ will be used as a predictor. We delete the predictor from
    our list because we can use it only once.

3.  The algorithm takes p+1 and creates a linear model. The p with the
    highest RSQ will be used as a predictor. Select the model with the
    highest RSQ and calculate the AIC.

4.  Iterate step 3 until the AIC of the model does not decrease anymore.

5.  We have found the (presumably) optimal model.

For the first task, we apply only the first 2 steps because we want only a
bivariate model. For the second task, we iterate step 3. If we do that, we
might also be able to choose the best multivariate model.

## R: Evaluation of the data

The open-source program R-Studio (Version 2022.12.0+353) was used for all
studies. The data handling in this program is done using the R language. We
utilize the R-package "Tidyverse" and other packages to process the data
effectively.

# Programming and data evaluation

## Packages

The following code chunk contains all packages we need. Important is the
package "conflicted". It enables us to choose if different functions have
the same call, but do not make the same thing (a function in a certain
package can have the same name as a function from another package). In this
case, we will set preferences.

```{r Packages, error=FALSE, message=FALSE, warning=FALSE}
# Load the file
source("../../R/general/packages.R")
```

## Read the file

First, we get the data and save it in our repository. But we must do this
only once. If the file exists, then we can read it directly. That is why we
implemented an if-else statement.

```{r Read_file, error=FALSE, message=FALSE, warning=FALSE}
# We want to know, if a certain file already exists
name.of.file <- "../../data/re_stepwise/FLX_CH_stepwise.csv"

# If do not exists such a file, create it!
if (!file.exists(name.of.file)){
  # Access to the data
  url <- "https://raw.githubusercontent.com/geco-bern/agds/main/data/df_for_stepwise_regression
  .csv"
  
  # Read the data directly from an URL
  database.S1 <- read.table(url, header = TRUE, sep = ",")

  # Write a CSV file in the repository
  write_csv(database.S1,"../../data/re_stepwise/FLX_CH_stepwise.csv")
  
  # Read the file
  database <- read_csv("../../data/re_stepwise/FLX_CH_stepwise.csv")

  # If exists such a file, read it only!
  }else{database <- read_csv("../../data/re_stepwise/FLX_CH_stepwise.csv")}
```

## Data Overview

### Missing values

To make a good decision about the predictors, we need as much information
as possible. For that, we read our file as a data frame. After that, we
want an overview of the data (what variables contain the data set, and what
are the main statistical parameters?)

```{r Read_the_data, error=FALSE, message=FALSE, warning=FALSE}
# Check wheater there are any missing values (We try with only the first six rows)
head(apply(database,2, function(x) is.na(x)))

# Load the function
source("../../R/general/function.visualize.na.values.R")

# Function call to visualize the NA
visualize.na.values.without.groups(database)

# Take an overview and main statistic parameters
summary(database)
```

## Bivariate linear regression

We chose the best variable for our predictor. For that, we tried all
variables and chose the one with the highest RSQ (see introduction).

```{r Calculate_best_Bivariante_Model, error=FALSE, message=FALSE, warning=FALSE}
# Access the outsourced function for the bivariate model
source("../../R/re_stepwise/function.bivariate.model.R")

# Function call
tibble.bi.model <- model.fitter(database, 16)
knitr::kable(tibble.bi.model)
```

### Visualization of the bivariate model

There are meaningful scatterplots for bivariate LM models, and they give us
a first impression. But we want to know more about the coefficients, the
intercept, the distribution, and the outliers. For that, we use summary()
and plot the model (see discussion).

```{r Vis_bivariate_regression_model, error=FALSE, message=FALSE, warning=FALSE}
# Calculate the probebly best multivariate model
best.bi.lm <- lm(GPP_NT_VUT_REF~PPFD_IN, data = database)

# Load the function
source("../../R/re_stepwise/function.vis.bi.model.R")

# Plot the bivariate model
vis.bi.model(database)

# Overview about all important model parameter
# Model Call. We are also interested for the p-values
summary(best.bi.lm)

# Plot the model 
plot(best.bi.lm)
```

### Development of RSQ

What is happening with RSQ? Here we visualize the RSQ for each variable. We
can easily see that PPFD_IN has the highest RSQ, and that is why we chose
it as our predictor. But what if we want a multivariate regression? One
might think that we should simply choose the variable with the
second-highest RSQ. But we will see that this is generally not the case.

```{r Vis_RSQ_development, error=FALSE, message=FALSE, warning=FALSE}

# Load the function
source("../../R/re_stepwise/function.vis.and.dev.bi.R")

# Function call
Vis_of_the_develop.bi.model(tibble.bi.model)
```

## Multivariate linear regression

Here we want to answer the question of which predictors we should choose to
get the probably best linear regression model. For that, we have written a
function. It works as described in the introduction. We make two function
calls. For the first, we use all variables but the target. This is,
however, not necessarily the best way. We could say that the time factor
and the location are irrelevant, so we drop these variables. We can see
that the difference is huge.

```{r best_multivariate_model, error=FALSE, message=FALSE, warning=FALSE}
# Access the outsourced function of the multivariate model
source("../../R/re_stepwise/function.multivariate.model.R")

# Function call with all column but target (column number 16)
multi.model.1 <- multivariate.model(database, c(16))

# Function call without the variables siteid, TIMESTAMP and GPP_NT_VUT_REF
multi.model.2 <- multivariate.model(database, c(1,2,16))

# Overview about the models
knitr::kable(multi.model.1)
knitr::kable(multi.model.2)
```

### Development of RSQ and AIC

We visualize the development of the model. We can see that the model with
more variables has a higher RSQ. That is not a surprise because RSQ will
always increase if we add a variable. Important is the AIC. We can see that
the AIC is lower for more variables (21762.2 vs. 24469.3). The RSQ is
higher (0.6579 vs. 0.5983). It follows that model 1 is better than model 2.
We see that also in the coefficients.

```{r Vis_RSQ_AIC_development, error=FALSE, message=FALSE, warning=FALSE}
# Load the function
source("../../R/re_stepwise/function.vis.and.dev.multi.R")

# Function call
vis.and.dev.multi.model(multi.model.1)
vis.and.dev.multi.model(multi.model.2, c("[without siteid and TIMESTAMP]"))
```

### Visualization of the multivariate model

There are no meaningful scatterplots for multidimensional models. But we
want to know more about the coefficients, the intercept, the distribution,
and the outliers. We make some calls by using summary(). We get the same
values for the RSQ. This is proof that our algorithm works. We see that not
all coefficients are significant.

```{r Visn_multivariate_regression_model, error=FALSE, message=FALSE, warning=FALSE}
# Calculate the probably best multivariate model
best.multi.lm <- lm(GPP_NT_VUT_REF~PPFD_IN + LW_IN_F + VPD_F + TA_F_MDS + 
                      SW_IN_F + P_F + WS_F + CO2_F_MDS + PA_F + siteid + TIMESTAMP, 
                    data = database)

second.multi.lm <- lm(GPP_NT_VUT_REF~PPFD_IN + LW_IN_F + VPD_F + TA_F_MDS + 
                      SW_IN_F + P_F + WS_F + CO2_F_MDS + PA_F, 
                    data = database)

# Overview about all important model parameter
# Model Call
summary(best.multi.lm)

summary(second.multi.lm)

# Plot the model 
plot(best.multi.lm)
```

# Discussion

## General

It seems that the step-forward algorithm has been successfully implemented.
However, it is important to choose the predictors very carefully because
the final model could vary. The target, of course, must be removed before
we can use the algorithm because if we use the target as a predictor, RSQ
will always be 1. Additionally, we must be aware of linear combinations. We
must always know what each variable stands for. The linear combination
could disrupt our results. After that, we must decide whether the variable
is a meaningful predictor. For example, a time factor could be very
important. Maybe the experiment setting was outdoors and the climate
factors were important. It could also be possible that our experimental
setting was indoors and there were always labor conditions. That means,
that the date and time are irrelevant. It is not always clear what a "good
choice" is.

For task 1, we could have written a very similar function as for task 2.
But we wrote a function that is a little bit more complicated than
necessary. We did this to demonstrate how the algorithm works. The
algorithm calculates a linear regression model for each predictor. With the
table, we are able to see that the model with the highest RSQ is not
necessarily the model with the lowest AIC.

We also wrote a function for task two. It is a step-forward algorithm.
First, the function calculates a bivariate linear regression model and
chooses the one with the highest RSQ. After that, a multivariate regression
model is calculated, and again, the one with the highest RSQ is chosen. For
each round, we compare the AIC. If the AIC is higher than the model before,
we stop the calculation, and we have probably found our best-fit model. But
here we have the same problem as described above. Our calculation depends
on our input. Therefore, we need to consider which variables we include in
the calculation.

For demonstration, we made some function calls. We can easily see that the
results are different. That means we must (1) choose wisely (maybe with an
educational guess), (2) try different calls to be able to estimate how big
the difference is, and (3) document how and why we have decided.

## Discussion of plot(model) call

Inspired by: <https://www.statology.org> and the book: "Statistik" form L.
Fahrmeir

### For the bivariate model

Residuals vs. fitted plot

We can see the right line follows almost the dashed line. It is near zero
and approximately a horizontal line. That means the linearity is not
violated and the assumption that we use a linear regression model was ok.

Q-Q-Plot

The Q-Q-Plot shows us that the residuals are almost following a normal
distribution. The error is probably mainly statistical and not systematic.
The plot also shows that the choice of a linear model was the right one
because it follows almost the line with a 45° angle (the values would then
have a perfect normal distribution).

Scale-Location Plot

The red line should be roughly horizontal across the plot. If so, then the
assumption of homoscedasticity is probably right. The scattering should be
random, and there should be no pattern at all. Both are approximately true.
The spread of the residuals is roughly equal at all fitted values.

Leverage-Plot

There are no values behind the cook's distance. That means that there are
no significant outliers that distort our analysis.

### For the multivariate model

The plots are approximately the same as for the bivariate model and need no
more discussion. Only the scale-location plot needs a few words: The red
line is not horizontal. That means there could be a problem with
homoscedasticity. But it is not so dramatic and does not need an
intervention. But we should keep this in mind.

## Development of RSQ and AIC

We can see that RSQ increases quickly for the first few steps. But then it
slows down, and suddenly there is no further improvement, and we can stop
our model. For the AIC, we can see similar behavior. But AIC decreases
instead of increasing. First, the decrease is fast and slows down. In the
last step, the AIC increased a bit. That is when we stop our calculation.

The RSQ is based on the variance decomposition theorem, which states that
the variance of the dependent variable can be written as the sum of a part
of the variance explained by the regression model and the variance of the
residuals (unexplained variance). By definition, it increases with
additional variables, but the growth flattens out. Actually, we should
discuss the adjusted RSQ instead of the RSQ. The RSQ is always a bit higher
than the adjusted RSQ. But it does not matter which one we use in the
algorithm. The algorithm stops if the AIC increases. If we choose a final
model, then we have to use the adjusted RSQ. AIC is a kind of quality
measure for the models. It is based on log-likelihood and a correction term
that takes into account the number of variables. In the beginning, AIC
becomes smaller. When the influence of the correction term becomes too
large, the AIC does not get smaller anymore, and we have a basis for
decision-making for the model choice.

It is also important to note that RSQ does not simply add up but must be
recalculated constantly. This means that for a multivariate model, we
cannot simply use the highest RR of the individual bivariate models or the
variables with the highest correlation coefficients. This is visible in the
figures "visualization of the development of RR and AIC (bivariate). That
is why we have to use the AIC for a good model choice, and that is also why
we recalculate the RR in our step-forward algorithm in each round.

---
title: "Report Exercise: re_ml_02"
author: "Patrick Bigler"
date: "2023-04-24"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
editor_options: 
  markdown: 
    wrap: 75
---

Course: Applied Geo-data Science at University of Bern (Institute of
Geography)

Supervisor: Prof. Dr. Benjamin Stocker

Adviser: Dr. Koen Hufkens, Pepa Aran, Pascal Schneider

Further information: <https://geco-bern.github.io/agds/>

[You have questions to the workflow? Contact the Author:]{.underline}

Author: Bigler Patrick
([patrick.bigler1\@students.unibe.ch](mailto:patrick.bigler1@students.unibe.ch){.email})

Matriculation number: 20-100-178

Reference: Report Exercise 7 (Chapter 10)

# Introduction

## Objectives

Here, we explore the role of structure in the data for model
generalisability and how to best estimate a "true" out-of-sample error that
corresponds to the prediction task. The task here is to train a model on
ecosystem flux observations from one site and predict to another site
(spatially upscaling).

-   Concept of cross validation

-   spatially upscalling

## Theory

The theory is very similar to re_ml_01. We change only our method:

# Method

## R: Evaluation of the data

All analysis are done with the open source software R-Studio (Version
2022.12.0+353). This software use the language R to handle the data. For an
efficient processing of the data we use the R-package "Tidyverse (and
others).

# Programming and data evaluation

## Packages

The following code chunk contains all packages we need. Important is the
package "conflicted". It enables us to chose if different functions have
the same call but do not make the same thing (a function in a certain
package can have the same name as a function from another package).

```{r Load_packages, error=FALSE, message=FALSE, warning=FALSE}
source("../../R/general/packages.R")
```

## Read the file

Use the URL in the code chuck to get access to the data. We use an if/else
statement to ensure that we do not overwrite the data with every run. If
the file exists, we read it local.

```{r Load_and_read_data, error=FALSE, message=FALSE, warning=FALSE}
name.of.file <- "../../data/re_ml_02/daily_fluxes.davos.csv"

# If do not exists such a file, create it!
if (!file.exists(name.of.file)){
  # Access to the data
  url.1 <- "https://raw.githubusercontent.com/geco-bern/agds/main/data/FLX_C
  -Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv"
  # Read in the data directly from URL
  daily_fluxes.davos <- read.table(url.1, header = TRUE, sep = ",")
  # Write a CSV file in the respiratory
  write_csv(daily_fluxes.davos, "../../data/re_ml_02/daily_fluxes.davos.csv")
  # Read the file
  daily_fluxes.davos <- read_csv("../../data/re_ml_02/daily_fluxes.davos.csv")
  # If exists such a file, read it only!
  }else{daily_fluxes.davos <- read_csv("../../data/re_ml_02/daily_fluxes.davos.csv")}

name.of.file <- "../../data/re_ml_02/daily_fluxes.laegern.csv"

# If do not exists such a file, create it!
if (!file.exists(name.of.file)){
  # Access to the data
  url.2 <- "https://raw.githubusercontent.com/geco-bern/agds/main/data/FLX_CH
  -Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv"
  # Read in the data directly from URL
  daily_fluxes.davos <- read.table(url.1, header = TRUE, sep = ",")
  # Write a CSV file in the respiratory
  write_csv(daily_fluxes.laegern, "../../data/re_ml_02/daily_fluxes.laegern.csv")
  # Read the file
  daily_fluxes.laegern <- read_csv("../../data/re_ml_02/daily_fluxes.laegern.csv")
  # If exists such a file, read it!
}else{daily_fluxes.laegern <- read_csv("../../data/re_ml_02/daily_fluxes.laegern.csv")}
```

## Data Overview

### Data cleaning

We can see that some columns contains -9999 as a value. Our quality
function changed that to NA. Than we use ymd() from the "lubridate" package
to rewrote the date in a proper way. Further, we want only columns which
contains good quality. For that we check selected columns with their
quality control columns. If the proportion of good measurement is less than
80%, then we overwrite the value with NA (and do not drop the row!). Now we
know that our data has a high quality and we can perform our analyses with
it.

Here, we work with two location (Davos and Lägern). But we do not want to
do everything twice. That why we define a new data frame which contains all
information about the two locations. We use .id = "id" because we must know
where the values come from. If we know that, we can use the filter function
from the package "dplyr" and analyse our data efficient.

```{r Dataquality, error=FALSE, message=FALSE, warning=FALSE}
# Load the function in the file (we use the function from another markdown again)
source("../../R/re_ml_01/function.use.good.quality.only.R")

# Function call to clean the data of Davos
daily_fluxes.davos <- use.good.quality(daily_fluxes.davos)
# Function call to clean the data of Lägern
daily_fluxes.laegern <- use.good.quality(daily_fluxes.laegern)

# We create a new data frame with bind_rows. We us "id" as an identifier. 
daily_fluxes_both <- bind_rows(daily_fluxes.davos, daily_fluxes.laegern, .id = "id")

# We check the again. Now our dataset contains NAs and only the columns of interest
# Load the function
source("../../R/general/function.visualize.na.values.R")

# Function call
visualize.na.values.without.groups(daily_fluxes_both)
```

## Split the data

Here we split our data in a training set and a test set. First we will
split the data (80 % training and 20 % test). We set seed for a
pseudo-random choice (reproducibility).

```{r error=FALSE, message=FALSE, warning=FALSE}
# For reproducibility (pseudo-random)
set.seed(123)  
# Split 80 % to 20 % 
split_davos <- rsample::initial_split(daily_fluxes_both|> 
                                        dplyr::filter(id == 1) , 
                                        prop = 0.8, strata = "VPD_F")

split_laegern <- rsample::initial_split(daily_fluxes_both|> 
                                          dplyr::filter(id == 2), 
                                          prop = 0.8, strata = "VPD_F")

split_both <- rsample::initial_split(daily_fluxes_both, prop = 0.8, strata = "VPD_F")

# Split Davos
daily_fluxes_davos_train <- rsample::training(split_davos)
daily_fluxes_davos_test <- rsample::testing(split_davos)

# Split Lägern
daily_fluxes_laegern_train <- rsample::training(split_laegern)
daily_fluxes_laegern_test <- rsample::testing(split_laegern)

# Split pooled
daily_fluxes_both_train <- rsample::training(split_both)
daily_fluxes_both_test <- rsample::testing(split_both)
```

## Find optimal k

We us a sequence to determine the optimal k. The model with a optimal k has
the smallest mean absolute error (MAE). But we are also interest in other
metrics like RSQ. Our function will read RSQ as well. We create three
models.

1.  We train a model with data from Davos
2.  We train a model with data from Lägern
3.  We train a model with data from Davos+Lägern

### Davos

We can see, that the optimal k is 59. This seems odd because the data are
the same as for the exercise re_ml_01 snd there was our optimal k = 25. But
we have to consider that we use a different approch to create our model. We
use "cross validation" which is why we find different optimal k for the
same data (and same set.seed).

```{r error=FALSE, message=FALSE, warning=FALSE}
source("../../R/re_ml_01/function.parameter.extracter.R")

source("../../R/re_ml_01/function.train.and.test.R")

# Define a sequence for k. Use 1,2,3,4 to show the curve at the beginning
my.sequence <- c(1, 2, 3, 4, seq(5, to = 100, by = 5), 59)

# Visualize the MAE and RSQ --> optimal k is 25 (between 20:30)
parameter.extracter(my.sequence, daily_fluxes_davos_train, daily_fluxes_davos_test)
```

### Lägern

We make the same thing for Lägern. Here is optimal k equal to 25. We
visualize the main metrics again.

```{r error=FALSE, message=FALSE, warning=FALSE}
source("../../R/re_ml_01/function.parameter.extracter.R")

source("../../R/re_ml_01/function.train.and.test.R")

# Define a sequence for k. Use 1,2,3,4 to show the curve at the beginning
my.sequence <- c(1, 2, 3, 4, seq(5, to = 100, by = 5))

# Visualize the MAE and RSQ --> optimal k is 25 (between 20:30)
parameter.extracter(my.sequence, daily_fluxes_laegern_train, daily_fluxes_laegern_test)
```

### Davos + Lägern

We do the same thing for the pooled data and we find a optimal k equal to
38.

```{r error=FALSE, message=FALSE, warning=FALSE}
source("../../R/re_ml_01/function.parameter.extracter.R")

source("../../R/re_ml_01/function.train.and.test.R")

# Define a sequence for k. Use 1,2,3,4 to show the curve at the beginning
my.sequence <- c(1, 2, 3, 4, seq(5, to = 100, by = 5))

# Visualize the MAE and RSQ --> optimal k is 25 (between 20:30)
parameter.extracter(my.sequence, daily_fluxes_both_train, daily_fluxes_both_test)
parameter.extracter(c(30:40), daily_fluxes_both_train, daily_fluxes_both_test)

```

## Best models

Now we know the optimal k for each model and we recalculate them...

```{r error=FALSE, message=FALSE, warning=FALSE}
source("../../R/re_ml_02/function.knn.cv.model.R")

# Function call for Davos
knn.model.davos.optimal <- knn.cv.model(daily_fluxes_davos_train, 59, 10)
# Function call for Lägern
knn.model.laegern.optimal <- knn.cv.model(daily_fluxes_laegern_train, 25, 10)
# Function call for Davos and Lägern
knn.model.both.optimal <- knn.cv.model(daily_fluxes_both_train, 38, 10)
```

### Visualization of the models

...and visualize our findings.

```{r error=FALSE, message=FALSE, warning=FALSE}
# Load the function
source("../../R/re_ml_01/function.evaluation.model.R")

# Model Davos
P_1 <- eval_model(knn.model.davos.optimal, daily_fluxes_davos_train, 
                  daily_fluxes_davos_test, 
                  c("Davos (opt. k = 59)"), c("Davos (opt. k = 59)"))

P_2 <- eval_model(knn.model.davos.optimal, daily_fluxes_davos_train, 
                  daily_fluxes_laegern_test, 
                  c("Davos (opt. k = 59)"), c("Lägern (opt. k = 59)"))

P_3 <- eval_model(knn.model.davos.optimal, daily_fluxes_davos_train, 
                  daily_fluxes_both_test, 
                  c("Davos (opt. k = 59)"), c("Davos and Lägern(opt. k = 59)"))

# Model Lägern
P_4 <- eval_model(knn.model.laegern.optimal, daily_fluxes_laegern_train, 
                  daily_fluxes_davos_test, 
                  c("Lägern (opt. k = 25)"), c("Davos (opt. k = 25)"))

P_5 <- eval_model(knn.model.laegern.optimal, daily_fluxes_laegern_train, 
                  daily_fluxes_laegern_test,
                  c("Lägern (opt. k = 25)"), c("Lägern (opt. k = 25)"))


P_6 <- eval_model(knn.model.laegern.optimal, daily_fluxes_laegern_train, 
                  daily_fluxes_both_test, 
                  c("Lägern (opt. k = 25)"), c("Davos+Lägern (opt. k = 25)"))

# Model Davos und Lägern
P_7 <- eval_model(knn.model.both.optimal, daily_fluxes_both_train,
                  daily_fluxes_davos_test, c("Davos and Lägern (opt. k = 38)"), 
                  c("Davos (opt. k = 38)"))

P_8 <- eval_model(knn.model.both.optimal, daily_fluxes_both_train, 
                  daily_fluxes_laegern_test, c("Davos and Lägern (opt. k = 38)"), 
                  c("Lägern (opt. k = 38)"))


P_9 <- eval_model(knn.model.both.optimal, daily_fluxes_both_train,
                  daily_fluxes_both_test, c("Davos and Lägern (opt. k = 38)"), 
                  c("Davos+Lägern (opt. k = 38)"))

# Plots
cowplot::plot_grid(P_1, P_2, P_3, ncol = 1)

cowplot::plot_grid(P_4, P_5, P_6, ncol = 1)

cowplot::plot_grid(P_7, P_8, P_9, ncol = 1)
```

### Residue for Davos

The function is very flexible. If we do not specifies which months we want,
then the function will use all months. Now we can analyse weather there are
any patterns. First we take a look over the whole year. We can see, that
the model underestimate the GPP-values in Davos. For Lägern, the model
overestimate the GPP-value. To explain that, we need a higher resolution
(monthly). we take a closer look to the seasons (DJF, MAM, JJA, SON). The
function calculate the difference between the measured GPP and the fitted
GPP.

```{r error=FALSE, message=FALSE, warning=FALSE}
# Load the function into the file
source("../../R/re_ml_02/function.vis.residue.R")

# Residue for Davos: train: pooled, test: Davos (year)
time.variation.year(knn.model.both.optimal, daily_fluxes_davos_test,
                    c("Train: Pooled, Test: Davos, k = 38"), 
                    c("re_ml_2 (Chapter 10)"))

# Residue for Davos: train: pooled, test: Davos (monthly)
time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_davos_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"))

time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_davos_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"), c("December", "January", "February"))

time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_davos_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"), c("March","April", "May"))

time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_davos_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"), c("June","July", "August"))

time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_davos_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"), c("September","October", "November"))
```

### Residue for Lägern

We do the same for Lägern

```{r error=FALSE, message=FALSE, warning=FALSE}

# Residue for Läger: train: pooled, test: Lägern (year)
time.variation.year(knn.model.both.optimal, daily_fluxes_laegern_test,
                    c("Train: Pooled, Test: Lägern, k = 38"), 
                    c("re_ml_2 (Chapter 10)"))

# We predict Lägern with pooled
time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_laegern_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Lägern, k = 38"),
                     c("re_ml_02 (Chapter 10)"))

# We predict Lägern with pooled
time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_laegern_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Lägern, k = 38"),
                     c("re_ml_02 (Chapter 10)"),c("December","January", "February"))

time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_laegern_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"), c("December", "January", "February"))

time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_laegern_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"), c("March","April", "May"))

time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_laegern_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"), c("June","July", "August"))

time.variation.ml.02(knn.model.both.optimal, 
                     daily_fluxes_laegern_test,c("KNN-Model:"), 
                     c("Train: Pooled, Test: Davos, k = 38"),
                     c("re_ml_02 (Chapter 10)"), c("September","October", "November"))
```

# Discussion

## Location

We have merged two data sets. But the two locations are very different.

| Quantity                         | Davos                 | Lägern            |
|----------------------------------|-----------------------|-------------------|
| Coordinates^1^:                  | 46.81°N, 9.84°E       | 47.48°N, 8.4°E    |
| Elevation^1^:                    | 1594 m.a.s.l.         | 873 m.a.s.l.      |
| Exposure^1^:                     | South-East slope      | Summit            |
| 2m Temperature^1^ (1990-2020)    | 3.8°C - 5.3°C         | 6.2°C - 9.4°C     |
| Precipitation^1^ (1990-2020      | 1046 mm per year      | \-                |
| Hours of sunshine^1^ (1990-2020) | 1537 h - 2076 h       | 1462 h - 2152 h   |
| Wind^1^ (1990-2020)              | 6 .8 km/h - 10.4 km/h | 15 km/h - 18 km/h |
| Vegetation^2^:                   | Coniferous forests    | Deciduous forests |

: Table 1: Comparison of the measuring stations (Davos vs. Lägern)

## Model

### Interpretation pooled train data applied on test data

### Interpret biases of the out-of-sample predictions with a view to the site characteristics

We tried different approaches to modelling the GPP. We took some metrics
into account (e.g. RSQ, MAE) and optimized k. Our goal was to create a
model which is generalizable. That means, that we want a model which can
predict the same variable (GPP) in different locations (Davos, Lägern).
After we have found the probably best model, we applied the model to
predict the test data. The difference between predicted test data and the
test data is called the bias. With this parameter we are able to discuss
our analysis.

A very important parameter is the bias. We can see, that our model
underestimate the GPP for Davos. If we group the data in months then we can
see that the model underestimate every month. However, not every month has
the same dispersion. In the winter-season (DJF) is the dispersion smaller
than in spring (MAM).

The bias for Lägern is positive. Therefore, the model overestimate the GPP
in Lägern. But if we take a monthly resolution we can see, that this is
true, but the amplitude varies. For example overestimate the model GPP in
summer (JJA) by almost $1.8\:\mu mol*m^{-2}*s^{-1}$ and for all other
months almost zero.

The reason for that is not clear. It could be the elevation. In Davos, the
dispersion starts with April. The later snowmelt could be responsible for
this. In Lägern not so much snowfall is measured. The dispersion start in
February. That means that the the vegetation is in Lägern earlier "active"
than in Davos.

Another explanation could be the exposure. In Lägern, the station is at a
"summit". That means there are more wind. Maybe the wind falsify the
measurement (But we do not think that because that would be very
unprofessional).

The vegetational context could also be a explanation. Decidious

# Bibliography

[(1) Meteorological Data]{.underline}

Data for Davos:
<https://www.meteoschweiz.admin.ch/service-und-publikationen/applikationen/messwerte-und-messnetze.html#param=messnetz-automatisch&lang=de&station=DAV&chart=year>

Data for Lägern:
<https://www.meteoschweiz.admin.ch/service-und-publikationen/applikationen/messwerte-und-messnetze.html#param=messnetz-automatisch&lang=de&station=LAE&chart=year>

[(2) Vegetation]{.underline}

[https://de.wikipedia.org/wiki/Höhenstufe\_%28Ökologie%29](https://de.wikipedia.org/wiki/Höhenstufe_%28Ökologie%29)

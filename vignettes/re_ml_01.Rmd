---
title: "Report_Exercise_4"
author: "Patrick Bigler"
date: "2023-05-01"
output: html_document
---

```{r Packages}
source("../R/packages.R")
```


```{r Read_the_file}
# Quiet a message because we know it already
options(readr.show_col_types = FALSE)

# Read the file
daily_fluxes <- data.frame(read_csv("../data/DAV_Fluxnet_2015.csv"))

# Take a closer look to the missing values
visdat::vis_miss(
  daily_fluxes,
  cluster = FALSE, 
  warn_large_data = FALSE)
```


```{r Dataquality}
# Load the function in the file
source("../R/Exercise_4/function.use.good.quality.only.R")

# Function call
daily_fluxes <- use.good.quality.only(daily_fluxes)
```


```{r splitting_data_in_train_and_test}
# Load the function in the file
source("../R/Exercise_4/function.split.train.and.test.R")

# Function call for KNN with k = 8
mod.knn <- knn.model(8)

# Function call for lm
mod.lm <- lm.model()
```


```{r model_evaluation}
# Load the function
source("../R/Exercise_4/function.evaluation.model.R")

# Function call for linear regression model
eval_model(mod = mod.lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)

# Function call for KNN
eval_model(mod = mod.knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```


```{r visualization_time_variation}
daily_fluxes|>
  dplyr::filter(rownames(daily_fluxes %in% daily_fluxes_train))


total <- merge(daily_fluxes, daily_fluxes_train,by="TIMESTAMP")

x <- total|>
  select(ends_with("x"))

y <- total|>
  select(ends_with("y"))
z <- x-y


```


```{r visualization_of_generalisability}
# Load the function into the file
source("../R/Exercise_4/function.different.k.R")

my.sequence <- c(1,2,3,4,seq(10, to = 90, by = 10), 96,97,98,99,100)
different.k(my.sequence, daily_fluxes_train, daily_fluxes_test)
```


Discussion

local regression model

Interpret observed differences in the context of the bias-variance trade-off:

Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?

- local regression
- MLM uses all predictors to calculate a regression, KNN uses only the nearest ones. 
- KNN use all values in the training sets and therefore it is always at least similar good


Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?
How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?

- because there are not a linear correlation
- KNN is trained locally

Lm have no variance

Lm do not have a bias because it is the definition of a bias

KNN will have a bias and variance and a balanced tradeoff.



Visualize temporal variations of observed and modlled GPP for both models, covering all available dates.


Let’s look at the role of k in a KNN. Answer the following questions:


state a hypothesis for how the R2 and the MAE evaluated on the test and on the 
training set would change for k approaching 1 and for k approaching N.

Hypothesis:

Let k = 1. Then the training data would be perfectly predicted. The bias would be zero.
But the model would be totally overfitted. That means if we we want 


Let k = N. 

Conclusion: There is always a bias-variance trade-off. 

Visualize results, showing model generalisability as a function of model complexity. 
Describe how a “region” of overfitting and underfitting can be determined in your 
visualization. Write (some of your) code into a function that takes k as an input 
and  returns the MAE determined on the test set.

Is there an “optimal” k in terms of model generalizability?
Edit your code to determine an optimal k.

An optimal K depends on the task and it is very difficult to determine the best k.
If we want a model that works well for predictions then we choose the k which 
gives us the highest RR in the evaluation. If we want a optimal description of the 
data we have, we look for the K which gives us the highest RR in the training set.
If we want a model with the best generalization, the we must have a close look 
to both RR.For that we calculate the difference between the RR from the training 
and the RR from the evaluation data set. The model with the smallest difference is 
the model with the probably best generalization.

High bias = underfitted

```{r}
# Load the function into the file
source("../R/Exercise_4/function.parameter.extracter.R")

# We define a sequence. each entry is a k and we iterate through the sequence
my.sequence.extracter <- c(1,2,3,4, seq(5, to = 100, by = 5))

# function call
plot(paramter.extracter(my.sequence.extracter, daily_fluxes_train))
```

```{r}

Metrics::mae(observed, predicted)

Metrics::mae(df$y, predict(model))
```


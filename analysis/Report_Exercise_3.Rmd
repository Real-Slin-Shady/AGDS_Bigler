---
title: "Exercise_3"
author: "Patrick Bigler"
date: "2023-04-24"
output: html_document
---


Introduction

The return of the function gives us a overview about the simple linear regression
models. 

Set the number of predictors to be considered to p equal 1.

Fit all regression models with p predictors and compute their R^2.

Select the model with p predictors that achieves the highest R^2 
(best fitting model) and compute its AIC.

Increment to p + 1. Fit all regression models with p + 1 predictors 
and compute their R^2. Select the best fitting model and compute its AIC.

If the AIC of the model with p + 1 predictors is poorer than the AIC of the model with  
p predictors, retain the model with p predictors and quit. 

You have found the (presumably) optimal model. Otherwise, continue with with step 4.

Packages

```{r Packages}
use_pkgs <-  c("dplyr", "tidyr", "readr", "lubridate", "stringr", "purrr",
              "ggplot2", "tidyverse", "visdat", "terra", "hexbin", "jsonlite",
              "MODISTools", "forcats", "yardstick", "recipes", "caret",
              "broom", "skimr", "cowplot", "scico", "hwsdr", "usethis",
              "renv", "rsample", "modelr", "rmarkdown", "rpart",
              "rpart.plot", "ranger", "sessioninfo", "ncdf4","ggcorrplot", 
              "corrr", "corrplot")

new_pkgs <- use_pkgs[!(use_pkgs %in% installed.packages()[, "Package"])]
if (length(new_pkgs) > 0) install.packages(new_pkgs)
invisible(lapply(use_pkgs, require, character.only = TRUE))
```


To decide which model we should choose

```{r Read_the_file}
# Read the file
database <- data.frame(read_csv("../data/halfhourly_data.csv"))

# Take a overview and main statistic parameters
summary(database)
```


First, we visualize our data. We need to decide what kind of regression we want. Linear? Polynomial or logarithmic? For that we make scatter plots. 

We can see, that a try with a linear regression model is a good idea. 
```{r Visualization_of_the_data}
# create a correlation matrix. Use "use" to ignore NAs
cor.mat <- cor(database[, 3:17], use = "complete.obs")

# plot a correlation plot. Now we have a good overview
cor.plo <- ggcorrplot(cor.mat, hc.order = TRUE, type  = "lower",
   lab = FALSE)
```


Bivariate linear regression

```{r Best_Bivariante_Model}
# Function to determine the best fit for a single regression model
model.fitter <- function(dataframe, target.column.nr){
  # Give all column-names to a vector
  col.names <- c(colnames(dataframe))
  # Create a empty vector. We will fill it with the RR for each model
  my.vec <- c()
  aic.vec <- c()
  # For loop over all columns
  for (i in c(1 : ncol(dataframe))) {
    # If the target and the predictor the same variable RR will be 1.
    # Therefore we use a if statement
    if (i != target.column.nr){
      lm.model <- lm(unlist(dataframe[target.column.nr]) ~ unlist(dataframe[i]))
      RR <- summary(lm.model)$r.squared
      aic.vec <- c(aic.vec, extractAIC(lm.model)[2])
      my.vec <- c(my.vec, RR )
    }
    # If the target and the predictor the same, than we want RR = AIC = 0
    else{
      my.vec <- c(my.vec, NA)
      aic.vec <- c(aic.vec, NA)
    }
  }
  # We create a tibble for a proper overview
  my.tibble <- print(tibble("Target" = rep(col.names[target.column.nr], 
                                     times = ncol(dataframe)),
                      "Predictor" = col.names,
                      "RR"  = my.vec,
                      "AIC" = aic.vec,
                      "Fit" = ifelse(RR >= max(RR, na.rm = TRUE), "BEST FIT", "NO")))
  return(print(my.tibble))
}

# Function call
df <- model.fitter(database, 16)
```

 Visualization of the bivariate model
 
There are no meaningful scatter-plot for multidimensional models. But we want to know
more about the coefficients, the intercept, the distribution and outliers. 

```{r Visualization_of_the_bivariate_regression_model}
# Calculate the probebly best multivariate model
best.bi.lm <- lm(GPP_NT_VUT_REF~PPFD_IN, data = database)

# Overview about all important model parameter

# Model Call
summary(best.bi.lm)

# Plot the model 
plot(best.bi.lm)

```

Multivariate linear regression

```{r Best_Multivariate_Model}
# function to find the best model fit (step forward algorithm)

# The function needs a data.frame and the column number of the target.
multivariante.model <- function(dataframe, vector){
  # Read all columns but the target and save their names in a vector
  col.names <- dataframe|>
  select(-vector) |>
  colnames()
  # Start with a empty vector. Here we will write down the predictors of the best fit for each round
  predictors <- NULL
  # Start with a empty vector for the column names. Here we will write down the fittest predictor for each round
  column.name <- NULL
  # Start with a big number. While loop should run till the AIC become bigger again
  AIC.old <- 9999999
  AIC.new <- 9999998
  while (AIC.new < AIC.old) {
    # Change the watcher for the while loop. We change the watcher for each round
    AIC.old <- AIC.new
    # Set the RR value to zero. We want a new start for each round in the for loop
    RR.now <- 0
    # col.names will be shorter for each round because we delete always the best fit from the vector
    for (i in col.names) {
      # For each round, we create a new formula for the linear regression model.   
      changeable.formula <- as.formula(paste("GPP_NT_VUT_REF~", paste(c(predictors, i), collapse = "+")))
      print(changeable.formula)
      # Calculate the linear regression model
      lm.model <- lm(changeable.formula, data = database)
      # Read R^2 
      RR.new <- summary(lm.model)$r.squared
      print(RR.new)
      # Search the highest R square and save the column name. We have to delete the column for the next round
      if(RR.new > RR.now){
        # Change the guard to enter the if condition
        RR.now <- RR.new
        # Extract the AIC. If the AIC bigger in the next round, we will overwrite it.
        AIC.new <- extractAIC(lm.model)[2]
        # We read the column name. If the AIC bigger for another predictor, we will overwrite it
        column.name <- as.character(i)
        } # end if condition
      # else does nothing
      else{NULL
        } # end else condition
      }# end for loop
    # Delete the name of the best fit of this round from the vector 
    col.names <- col.names[!col.names == column.name]
    # prepare the predictors for the next round
    predictors <- c(predictors, column.name)
    print(AIC.new)
    }# end while loop
# if we finished the while loop, we print our model  
print(paste("Best model fit has AIC =", AIC.old))
predictors <- predictors[-length(predictors)]
print(predictors)
return()
}

# Function call with all column but target (column number 16)
multivariante.model(database, c(16))

# Function call without the column numbers 1, 2 and 16
multivariante.model(database, c(1,2,16))
```

Visualization of the multivariate model

There are no meaningful scatter-plot for multidimensional models. But we want to know
more about the coefficients, the intercept, the distribution and outliers. 
```{r Visualization_of_the_multivariate_regression_model}
# Calculate the probebly best multivariate model
best.multi.lm <- lm(GPP_NT_VUT_REF~PPFD_IN + LW_IN_F + VPD_F + TA_F_MDS + 
                      SW_IN_F + P_F + WS_F + CO2_F_MDS + PA_F, data = database)

# Overview about all important model parameter

# Model Call
summary(best.multi.lm)

# Plot the model 
plot(best.multi.lm)
```

Discussion

It seems that the step forward algorithm is successfully implemented. However, 
it is important to choose the predictors very carefully because the final model 
could varies. The target, of course, must be removed before we can use the algorithm
because if we use the target as a predictor, R^2 will be always 1. Additional,
the predictor would be a linear combination of the target and therefore disrupt 
the results. 

All other column must be chosen very carefully as well. For that, we have to understand
for what the column stands. After that we must decide whether the variable is a meaningful
predictor. For example, could be time very important. Maybe the experiment-setting were
outdoor and the climate factors were important. It could also be possible, that our experiment
setting were indoor and there were always labor-conditions. That means, that the date and time
is irrelevant. It is not always clear what is a "good choice". 

For task 1 I could have wrote a very similar function as for task two. But I 
wrote a function which is a little bit complicated as necessary. I did
this to demonstrate how the algorithm works. The algorithm calculate a linear regression
model for each predictor. For the model with the highest R^2 w. You can see on the table
that the the model with the highest model is not necessary the model with the lowest AIC.

For task two I wrote also a function. It is a step forward algorithm. First, the function 
calculate a bivariate linear regression model and chose the one which has the highest
R^2. After that, a multivariate regression model is calculated and again
the one with the highest R^2 is chosen. For each round, we compare the AIC. If the AIC
higher than the model before, we stop the calculation and we have found our probebly 
best fit model. But here we have the same problem as describe above. Our calculation depends
on our input. We therefore need to consider which variables to include in the calculation.

To demonstrate that, I made different function calls. You can easely see that the results 
are different. That means we must (1) chose wise (maybe with educational guess)
(2) try different calls to be able to estimate how big the difference is and (3)
document how and why what was decided.




